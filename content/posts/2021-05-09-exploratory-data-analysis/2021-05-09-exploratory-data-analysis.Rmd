---
title: "Tagesthemen - Explorative Datenanalyse"
date: '2021-05-09'
keywords:
- Tagesthemen
- Explorative Datenanalyse
- R
tags: R
category: blog-post
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dpi = 300, cache.lazy = FALSE,
                      tidy = "styler", fig.width = 8, fig.height = 5)
library(tidyverse)
library(silgelib)
library(kableExtra)
theme_set(theme_plex())
update_geom_defaults("rect", list(fill = "midnightblue"))
```

## Einleitung

Im vorigen Post habe ich beschrieben, wie ich die Daten über die Tagesthemen-Sendungen gewonnen habe.
Nun geht es weiter und diese Daten werden einer ersten explorativen Datenanalyse unterzogen.

## Explorative Datenanalyse

Die Rohdaten aus dem Prozess des Scrapings können hier heruntergeladen werden:

[Moderator*innen](./2021-05-09-exploratory-data-analysis/moderatoren_sendung.csv)
[Sendungsdaten](.2021-05-09-exploratory-data-analysis/2021-05-09-exploratory-data-analysis/scraped_data.csv)

### Einlesen der Rohdaten & Sanity Checks

Dieser Schritt ist sehr einfach; als erstes werden die gescrapten Rohdaten mit den Informationen zur Sendung eingelesen und anschließend die Zuordnung der Moderator*innen zu den Sendungen. Im Folgeschritt werden beide über den Namen des Bildes (der eineindeutig ist) zusammengeführt und der nicht mehr benötigte DataFrame mit den Moderatoren gelöscht.

```{r}
library(tidyverse)

tagesthemen <- read_csv("scraped_data.csv")
moderatoren <- read_csv("moderatoren_sendung.csv")

tagesthemen <- tagesthemen %>% 
  mutate(dauer = as.numeric(dauer),
         date = as.Date(strftime(datum_zeit, format = "%Y-%m-%d")),
         dateiname_standbild = str_remove(standbild_url, "https://www.tagesschau.de/multimedia/bilder/")) %>% 
  left_join(moderatoren, by = c("dateiname_standbild" = "file"))

rm(moderatoren)

```

Jetzt wird die Vollständigkeit der Daten untersucht. 
Dazu nutze ich die Funktion "skim" aus dem Paket "skimr", weil der Output einen sehr guten Überblick über alle relevanten Kennzahlen gibt.

```{r}
library(skimr)

tagesthemen %>% 
  skim()
```
Mit Ausnahme von 7 Missings in der Variablen "name" sieht die Vollständigkeit schon mal gut aus.
Aber was ist bei der Variable los? Dazu filtere ich einmal die Missings an und schaue mir die Beobachtungen genauer an.

```{r}
tagesthemen %>% 
  filter(is.na(name)) %>% 
  select(-themen) %>% 
  kable()
```

Das macht es schon klarer: 4 der 7 Sendungen haben kein Vorschaubild (erkennbar an dem "NA" am Ende des strings); entsprechend kann die moderierende Person nur erkannt werden, in dem das Video analysiert/angeschaut wird.  

Normalerweise wäre das der Zeitpunkt, an dem man sich Gedanken machen müsste, wie man die Ableitung der moderierenden Person noch sicherer, also unabhängiger vom Vorschaubild, machen könnte. Aber das ist ein anderes Thema und wird hier nicht weiterverfolgt. Dennoch, hier ist er nun: der erste Scheideweg an dem man sich die Frage stellen muss, wie man weiter vorgeht. Was macht man mit diesen Missings? Es gibt meiner Meinung nach vier Optionen. 

### Option 1 (keinesfalls best practise) 
Man schaut sich die Videos an und hard-coded den Namen der moderierenden Person. Aber nehmen wir mal an, wir hätten z.B. 40 Missings - dann wäre das schon aufwändig.
Im Bereich Big Data hat man aber gelegentlich deutlich mehr missings, dann ist das nicht nur aufwändig sondern schlichtweg nicht mehr leistbar.

### Option 2 (finde ich persönlich zu einfach gedacht; findet man aber häufig) 
Die Beobachtungen werden gelöscht.

### Option 3 
Man ergänzt die Missings durch einen Platzhalter "Name nicht bekannt".

### Option 4 (dafür entscheide ich mich) 
Die Daten werden aufgefüllt. Hier kann man von einfacher (z.B. "Interpolation", KNN) bis sehr sophisticated (z.B. Random Forrest) herangehen. Ich entscheide mich im Folgenden für den einfacheren Weg, da der Datensatz für anspruchsvollere Ansätze noch zu wenig features aufweist.
Zum Auffüllen nutze ich die Funktion "fill" und habe mich für die Richtung "up" entschieden.

```{r}

tagesthemen <- tagesthemen %>% 
  arrange(datum_zeit) %>% 
  fill(name, .direction = "up")

```

Ein weiterer Blick auf das Data Summary weiter oben zeigt, dass noch das eine oder andere zu tun ist:

- Die Variable "Zeit" sollte gesplittet werden in "Datum" und "Uhrzeit"
- Die Sendedauer ist noch in Sekunden enthalten und sollte in Minuten umgerechnet werden, da Menschen besser in Minuten als Sekunden denken können
- Die Variable mit den Themen der Sendung muss noch optimiert werden; das wird aber später gemacht, wenn es um die Textanalyse geht

```{r}

tagesthemen <- tagesthemen %>% 
  mutate(date = as.Date(datum_zeit, format = "%Y-%m-%d"),
         time = format(datum_zeit, "%H:%M:%S"),
         dauer = dauer / 60)
```

Um die Qualität der Daten weiter zu überprüfen, frage ich mich, wann wurde die früheste Sendung der Tagesthemen ausgestrahlt?
Das lässt sich ganz leicht herausfinden.

```{r}
min(tagesthemen$time)
```

Die früheste Sendezeit einer Ausgabe der Tagestehmen ist also bereits um 21 Uhr. Hier kommt nun oft zitierte "Domain Expertise" ins Spiel und die sagt: es ist ungewöhnlich, dass eine Ausgabe der Tagesthemen bereits 1 Stunde nach der Tagesschau (um 20 Uhr) gesendet wird. Also müssen diese einmal genauer angeschaut werden.

```{r}
tagesthemen %>% 
  filter(time <= "21:00:00") %>% 
  select(-themen) %>% 
  kable()

```

Es existieren genau zwei Sendungen, die vor oder um 21 Uhr ausgestrahlt wurden. Auffällig ist, dass diese sehr kurz waren (maximal 8 Minuten). Ein Blick in die Aufzeichnung der Sendungen zeigt, dass es sich um Extra-Ausgaben handelte. Das heißt, es muss nun identifiziert werden, welche der Sendungen in dem Datensatz eine reguläre und welche eine Extra Ausgabe ist, denn es macht Sinn später danach zu differenzieren.

Zur Sicherheit prüfe ich noch einmal, wieviele Ausgaben der Tagesthemen an einem Tag ausgestrahlt wurden bilde darüber eine Häufigkeitstabelle.

```{r}
tagesthemen %>% 
  group_by(date) %>% 
  arrange(datum_zeit) %>% 
  summarize(anzahl_sendungen = n()) %>% 
  count(anzahl_sendungen)


```

An 779 Tagen gab es genau eine Ausgabe der Tagesthemen; an 17 Tagen gab es zwei Ausgaben.
Entsprechend werde ich im Fall von zwei Sendungen die kürzere als Extra-Ausgabe markieren.

```{r}
tagesthemen <- tagesthemen %>% 
  arrange(datum_zeit) %>% 
  group_by(date) %>% 
  mutate(extra = if_else(dauer == max(dauer),0,1))
```

Und nun kann schon eine erste grafische Inspektion erfolgen: wie verhält sich die Sendezeit der Tagesthemen über das Datum?

```{r}
tagesthemen %>% 
  ungroup() %>% 
  filter(extra == 0) %>%
  ggplot(aes(x=datum_zeit, y=dauer)) +
  theme_plex() +
  geom_line(color = "midnightblue") +
  geom_smooth() +
  labs(x = "Datum", y = "Sendezeit in Minuten", caption = "Quelle: www.tagesschau.de", 
       subtitle = "01.01.2019 - 16.03.2021 (ohne Extrausgaben) ", title = "Tagesthemen - Sendezeit nach Datum")

```

Das erste Ergebnis - und schon so interessant!

Zunächst ist starkes "Zucken" in den Daten erkennbar; in jeder Woche gibt es starke Schwankungen.
Daher erscheint es  sinnvoll, die Werte auch noch einmal aggregiert nach Wochentagen anzuschauen. Das passiert gleich.

Weiterhin ist erkennbar, dass ab ca. August 2020 eine kleine Treppe in der durchschnittlichen Sendezeit erkennbar ist. Das ist kein Fehler sondern hat nach meiner Recherche einen einfachen Grund: es wurde eine neue Serie in den Tagesthemen etabliert, namens "mittendrin". Mehr dazu in diesem Video: https://youtu.be/m6AGY1hB65o

Und natürlich fällt ein starker Extremwert ins Auge undzwar gleich zu Beginn des Jahres 2021. Hierbei handelt es sich um die Sendung vom 06. Januar 2021 als in den USA das Kongressgebäude gestürmt wurde.

Jetzt zu den Wochentagen. Für diese Analyse muss zunächst die Variable des Wochentages gebildet werden, um anschließend danach auswerten zu können. Damit in den Plots die Reihenfolge der Wochentage stimmt, wird die Reihenfolge der Faktor-Level explizit definiert und dann der Plot generiert.

```{r}

tagesthemen <- tagesthemen %>% 
  mutate(day = factor(weekdays(datum_zeit), levels = c("Montag", "Dienstag", "Mittwoch",
                                                       "Donnerstag","Freitag","Samstag","Sonntag"))) 

tagesthemen %>% 
  filter(extra == 0) %>% 
  group_by(day) %>% 
  summarise(mittlere_sendezeit = mean(dauer)) %>% 
  ggplot(aes(x = day, y = mittlere_sendezeit)) +
  geom_bar(stat = "identity", fill = "midnightblue") +
  labs(x = "Wochentag", y = "Sendezeit in Minuten", caption = "Quelle: www.tagesschau.de", 
       subtitle = "01.01.2019 - 16.03.2021 (ohne Extrausgaben) ", title = "Tagesthemen - Mittlere Sendezeit nach Wochentag") + 
  theme_plex()
```
Das die Tagesthemen am Wochenende kürzer sind, das wusste ich. Neu war mir, dass dies offensichtlich schon am Freitag der Fall ist. 
Erkennbar ist, das an den Tagen von Montag bis Donnerstag eine Sendung im Durchschnitt 28-30 Minuten dauert und von Freitag bis Sonntag knapp über 20 Minuten. Das ist doch gut zu wissen.

Mit diesen ersten Einblicken endet dieser Post zunächst.

In den nächsten Posts gehe ich auf die Suche nach weiteren Insights: z.B. nehme ich einmal den Dienstplan der Moderatoren unter die Lupe!